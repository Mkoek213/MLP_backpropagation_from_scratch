# MLP Language Model with Manual Backpropagation

This project implements a Multi-Layer Perceptron (MLP) language model, with backpropagation manually implemented from scratch. It is based on code from my previous project, [MLP_Language_Model](https://github.com/Mkoek213/MLP_language_model), and inspired by Andrej Karpathy's tutorial on [Building a GPT from Scratch](https://www.youtube.com/watch?v=q8SA3rM6ckI).

The complete code is available in `notebooks/model.ipynb`.

### Training and Validation Loss
- **Training Loss**: 2.1559
- **Validation Loss**: 2.1754

### Examples Generated by the Model
Below are some sample names generated by the model:
- mora
- mayanniellen
- hayla
- rethrucendraeg
- aderedoelin
- shi
- jenleigh
- can
- nar
- katelle
- kamin

---

## Code Overview

### 1. Importing Libraries and Preparing Data
```python
import torch
import torch.nn.functional as F
import numpy as np
import torch.nn as nn
import matplotlib.pyplot as plt
%matplotlib inline

words = open('names.txt').read().splitlines()
chars = sorted(list(set(''.join(words))))
stoi = {s:i+1 for i, s in enumerate(chars)}
stoi['.'] = 0
itos = {i:s for s, i in stoi.items()}
vocab_size = len(stoi)
```
This section imports the necessary libraries and prepares the vocabulary for character-based tokenization. Each character in names.txt is mapped to an integer, enabling efficient processing and embedding.


### 2. Building the Dataset
```python
import random
# build the dataset
block_size = 3

def build_dataset(words):
    X, Y = [], []

    for w in words:
        context = [0] * block_size
        for ch in w + '.':
            ix = stoi[ch]
            X.append(context)
            Y.append(ix)
            context = context[1:] + [ix]
    X = torch.tensor(X)
    Y = torch.tensor(Y)
    return X, Y

random.seed(42)
random.shuffle(words)
n1 = int(0.8 * len(words))  
n2 = int(0.9 * len(words))
Xtrain, Ytrain = build_dataset(words[:n1])
Xdev, Ydev = build_dataset(words[n1:n2])
Xtest, Ytest = build_dataset(words[n2:])
```
This code prepares the training, validation, and test datasets using a sliding window (of size block_size) over each word. The context is a list of indices representing characters.


### 3. Comparison Function for Manual vs. Autograd Gradients
```python
# func to compare our results 
def cmp(s, dt, t):
  ex = torch.all(dt == t.grad).item()
  app = torch.allclose(dt, t.grad)
  maxdiff = (dt - t.grad).abs().max().item()
  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')

n_embd = 10
n_hidden = 64

g = torch.Generator().manual_seed(2147483647)
C = torch.randn((vocab_size, n_embd), generator=g)
# layer 1
W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3) / ((n_embd * block_size) ** 0.5)
b1 = torch.randn(n_hidden, generator=g) * 0.1
# layer 2
W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1
b2 = torch.randn(vocab_size, generator=g) * 0.1
#batch norm
bngain = torch.randn((1, n_hidden))*0.1 + 1.0
bnbias = torch.randn((1, n_hidden))*0.1

parameters = [C, W1, b1, W2, b2, bngain, bnbias]
print(sum(p.nelement() for p in parameters))
for p in parameters:
    p.requires_grad = True
```
This helper function compares manually calculated gradients with gradients computed by PyTorch's autograd, allowing verification of the correctness of the manual backpropagation.


### 4. Forward Pass with Batch Normalization
```python
batch_size = 32
n = batch_size
ix = torch.randint(0, Xtrain.shape[0], (batch_size,), generator=g)
Xb, Yb = Xtrain[ix], Ytrain[ix]

# forward pass
emb = C[Xb]
embcat = emb.view(emb.shape[0], -1)

#linear layer 1
hprebn = embcat @ W1 + b1 #hidden layer pre-activation

# batch norm layer
bnmeani = 1 / n*hprebn.sum(0, keepdim=True)
bndiff = hprebn - bnmeani
bndiff2 = bndiff ** 2
bnvar = 1/(n-1) * bndiff2.sum(0, keepdim=True)
bnvar_inv = (bnvar + 1e-5) ** -0.5
bnraw = bndiff * bnvar_inv
hpreact = bngain * bnraw + bnbias

# non linearity
h = torch.tanh(hpreact)

# linear layer 2
logits = h @ W2 + b2

# cross entropy loss
logits_maxes = logits.max(1, keepdim=True).values
norm_logits = logits - logits_maxes
counts = norm_logits.exp()
counts_sum = counts.sum(1, keepdim=True)
counts_sum_inv = counts_sum ** -1
probs = counts * counts_sum_inv
logprobs = probs.log()
loss = -logprobs[range(n), Yb].mean()

# pytorch backward pass for comparision
for p in parameters:
    p.grad = None

for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, norm_logits, logits_maxes, logits, h, hpreact, bnraw, bndiff, bndiff2, bnvar, bnvar_inv, bnmeani, hprebn, embcat, emb]:
    t.retain_grad()
loss.backward()
```
Here, the model's forward pass includes batch normalization before the non-linearity. This helps stabilize training and provides better generalization.


### 5. Manual Backpropagation
```python
# backpropagation manually
dlogprobs = torch.zeros_like(logprobs)
dlogprobs[range(n), Yb] = -1.0 / n

dprobs = (1.0 / probs) * dlogprobs

dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)

dcounts_sum = -1 * counts_sum**(-2) * dcounts_sum_inv

dcounts1 = counts_sum_inv * dprobs
dcounts2 = torch.ones_like(counts) * dcounts_sum
dcounts = dcounts1 + dcounts2

dnorm_logits = norm_logits.exp() * dcounts

dlogits_maxes = torch.zeros_like(logits_maxes)
dlogits_maxes = -1 * dnorm_logits.sum(1, keepdim=True)

dlogits1 = torch.ones_like(logits) * dnorm_logits
dlogits2 = torch.zeros_like(logits_maxes)
dlogits2 = F.one_hot(logits.max(dim=1).indices, num_classes=logits.shape[1]) * dlogits_maxes
dlogits = dlogits1 + dlogits2

dh = dlogits @ W2.t()

dW2 = h.t() @ dlogits

db2 = dlogits.sum(dim=0)

dhpreact = (1.0 - h**2) * dh

dbnraw = bngain * dhpreact

dbngain = (bnraw * dhpreact).sum(0, keepdim=True)

dbnbias = dhpreact.sum(0, keepdim=True)

dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)

dbnvar = -0.5 * (bnvar + 1e-5)**-1.5 * dbnvar_inv

dbndiff2 = torch.ones_like(bndiff2) * 1/(n-1) * dbnvar

dbndiff1_1 = bnvar_inv * dbnraw
dbndiff1_2 = 2 * bndiff * dbndiff2
dbndiff = dbndiff1_1 + dbndiff1_2

dbnmeani = (-1) * dbndiff.sum(0, keepdim=True)

dhprebn1 = dbndiff.clone()
dhprebn2 = (1/n) * torch.ones_like(hprebn) * dbnmeani
dhprebn = dhprebn1 + dhprebn2

dembcat = dhprebn @ W1.t()

dW1 = embcat.t() @ dhprebn

db1 = dhprebn.sum(0, keepdim=True)

demb = dembcat.view(emb.shape)

dC = torch.zeros_like(C)
for row_idx in range(Xb.shape[0]):
    for col_idx in range(Xb.shape[1]):
        ix = Xb[row_idx,col_idx]
        dC[ix] += demb[row_idx,col_idx]

cmp('logprobs', dlogprobs, logprobs)
cmp('probs', dprobs, probs)
cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)
cmp('counts_sum', dcounts_sum, counts_sum)
cmp('counts', dcounts, counts)
cmp('norm_logits', dnorm_logits, norm_logits)
cmp('logits_maxes', dlogits_maxes, logits_maxes)
cmp('logits', dlogits, logits)
cmp('h', dh, h)
cmp('W2', dW2, W2)
cmp('b2', db2, b2)
cmp('hpreact', dhpreact, hpreact)
cmp('bnraw', dbnraw, bnraw)
cmp('bngain', dbngain, bngain)
cmp('bnbias', dbnbias, bnbias)
cmp('bnvar_inv', dbnvar_inv, bnvar_inv)
cmp('bnvar', dbnvar, bnvar)
cmp('bndiff2', dbndiff2, bndiff2)
cmp('bndiff', dbndiff, bndiff)
cmp('bnmeani', dbnmeani, bnmeani)
cmp('hprebn', dhprebn, hprebn)
cmp('embcat', dembcat, embcat)
cmp('W1', dW1, W1)
cmp('b1', db1, b1)
cmp('emb', demb, emb)
cmp('C', dC, C)
```
Calculates gradients for each step in the forward pass manually, avoiding autograd. This includes computing gradients for probabilities, normalization, logits, hidden layers, and batch normalization parameters.


### 6. Simplified Cross-Entropy Calculation
```python
# instead of forward passing step by step through cross entropy loss, we now use cross_entropy function, 
# and calculate derivative on piece of paper

# forward pass
loss_fast = F.cross_entropy(logits, Yb)
print(loss_fast.item(), 'difference: ', (loss_fast - loss).item())

#backward pass
dlogits = F.softmax(logits, dim=1)
dlogits[range(n), Yb] -= 1
dlogits /= n

cmp('logits', dlogits, logits)
```
This step demonstrates faster way of implementing backward pass for cross entropy.


### 7. Simplified Batch Normalization Calculation
```python
# instead of forward passing step by step throug batch normalization, we now use simpler form,
# and calculate derivative on piece of paper

# forward pass
hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias
print('max diff:', (hpreact_fast - hpreact).abs().max())

# bakcwrad pass
dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(dim=0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))
cmp('hprebn', dhprebn, hprebn)
```
This step demonstrates faster way of implementing backward pass for batch normalization layer.


### 8. Training Loop with Parameter Updates
```python
# putting everything together
n_embd = 10
n_hidden = 64


g = torch.Generator().manual_seed(2147483647)
C = torch.randn((vocab_size, n_embd), generator=g)
# layer 1
W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3) / ((n_embd * block_size) ** 0.5)
b1 = torch.randn(n_hidden, generator=g) * 0.1
# layer 2
W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1
b2 = torch.randn(vocab_size, generator=g) * 0.1
#batch norm
bngain = torch.randn((1, n_hidden))*0.1 + 1.0
bnbias = torch.randn((1, n_hidden))*0.1

parameters = [C, W1, b1, W2, b2, bngain, bnbias]
print(sum(p.nelement() for p in parameters))
for p in parameters:
    p.requires_grad = True

max_steps = 200000
batch_size = 32
n = batch_size
lossi = []

with torch.no_grad():
  for i in range(max_steps):

    ix = torch.randint(0, Xtrain.shape[0], (batch_size,), generator=g)
    Xb, Yb = Xtrain[ix], Ytrain[ix]

    # forward pass
    emb = C[Xb]
    embcat = emb.view(emb.shape[0], -1)

    # Linear layer
    hprebn = embcat @ W1 + b1

    # BatchNorm layer
    bnmean = hprebn.mean(0, keepdim=True)
    bnvar = hprebn.var(0, keepdim=True, unbiased=True)
    bnvar_inv = (bnvar + 1e-5)**-0.5
    bnraw = (hprebn - bnmean) * bnvar_inv
    hpreact = bngain * bnraw + bnbias

    # Non-linearity
    h = torch.tanh(hpreact)
    logits = h @ W2 + b2
    loss = F.cross_entropy(logits, Yb)

    # backward pass
    for p in parameters:
      p.grad = None

    #manual backward pass
    dlogits = F.softmax(logits, dim=1)
    dlogits[range(n), Yb] -= 1
    dlogits /= n
    dh = dlogits @ W2.t()
    dW2 = h.t() @ dlogits
    db2 = dlogits.sum(dim=0)
    dhpreact = (1.0 - h**2) * dh
    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)
    dbnbias = dhpreact.sum(0, keepdim=True)
    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(dim=0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))
    dembcat = dhprebn @ W1.t()
    dW1 = embcat.t() @ dhprebn
    db1 = dhprebn.sum(0)
    demb = dembcat.view(emb.shape)
    dC = torch.zeros_like(C)
    for row_idx in range(Xb.shape[0]):
        for col_idx in range(Xb.shape[1]):
            ix = Xb[row_idx,col_idx]
            dC[ix] += demb[row_idx,col_idx]
    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]

    # update
    lr = 0.1 if i < 100000 else 0.01
    for p, grad in zip(parameters, grads):
      p -= lr * grad

    if i % 1000 == 0:
      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')
    lossi.append(loss.log10().item())
```
This final section of the code puts everything together, using implemented from scratch backward pass. It runs the training loop for max_steps, calculating loss and updating parameters with gradient descent, adjusting the learning rate mid-way for optimization. 
